<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A clustering task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox" checked/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Data-preparation"><span>Data preparation</span></a></li><li><a class="tocitem" href="#Main-analysis"><span>Main analysis</span></a></li><li><a class="tocitem" href="#Working-without-the-labels"><span>Working without the labels</span></a></li><li><a class="tocitem" href="#Benchmarking-computational-efficiency"><span>Benchmarking computational efficiency</span></a></li><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Clustering - Iris</a></li><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="clustering_tutorial"><a class="docs-heading-anchor" href="#clustering_tutorial">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a><a id="clustering_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#clustering_tutorial" title="Permalink"></a></h1><p>The task is to estimate the species of a plant given some floreal measurements. It use the classical &quot;Iris&quot; dataset. Note that in this example we are using clustering approaches, so we try to understand the &quot;structure&quot; of our data, without relying to actually knowing the true labels (&quot;classes&quot; or &quot;factors&quot;). However we have chosen a dataset for which the true labels are actually known, so to compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris<em>flower</em>data_set</a></li><li>data source we use here: <a href="https://github.com/JuliaStats/RDatasets.jl">https://github.com/JuliaStats/RDatasets.jl</a></li></ul><p>!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.</p><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial</p><pre><code class="language-julia hljs">using BetaML
using Random, Statistics, Logging, BenchmarkTools, RDatasets, Plots, DataFrames</code></pre><p>We are also going to compare our results with two other leading packages in Julia for clustering analysis, <a href="https://github.com/JuliaStats/Clustering.jl"><code>Clustering.jl</code></a> that provides (inter alia) kmeans and kmedoids algorithms and <a href="https://github.com/davidavdav/GaussianMixtures.jl"><code>GaussianMixtures.jl</code></a> that provides, as the name says, Gaussian Mixture Models. So we import them (we &quot;import&quot; them, rather than &quot;use&quot;, not to bound their full names into namespace as some would collide with BetaML).</p><pre><code class="language-julia hljs">import Clustering, GaussianMixtures</code></pre><p>We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally</p><pre><code class="language-julia hljs">Random.seed!(123)
#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output</code></pre><p>Differently from the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>, we load the data here from [<code>RDatasets</code>](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.</p><pre><code class="language-julia hljs">iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
describe(iris)</code></pre><div class="data-frame"><p>5 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="DataType">DataType</th></tr></thead><tbody><tr><th>1</th><td>SepalLength</td><td>5.84333</td><td>4.3</td><td>5.8</td><td>7.9</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>SepalWidth</td><td>3.05733</td><td>2.0</td><td>3.0</td><td>4.4</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>PetalLength</td><td>3.758</td><td>1.0</td><td>4.35</td><td>6.9</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>PetalWidth</td><td>1.19933</td><td>0.1</td><td>1.3</td><td>2.5</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>Species</td><td></td><td>setosa</td><td></td><td>virginica</td><td>0</td><td>CategoricalValue{String, UInt8}</td></tr></tbody></table></div><p>The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values</p><h2 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h2><p>The first step is to prepare the data for the analysis. We collect the first 4 columns as our <em>feature</em> <code>x</code> matrix and the last one as our <code>y</code> label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we&#39;ll behave like we do not know them, we&#39;ll just let the algorithm &quot;learn&quot; fro mthe structure of the data itself. We&#39;ll however use it to judge the accuracy that they did reach.</p><pre><code class="language-julia hljs">x       = Matrix{Float64}(iris[:,1:4]);
yLabels = unique(iris[:,5]);</code></pre><p>As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the function <a href="../../Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractVector{T} where T}"><code>integerEncoder</code></a>.</p><pre><code class="language-julia hljs">y       = integerEncoder(iris[:,5],factors=yLabels);</code></pre><p>The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in crossValidation, but we are keeping here a copy of the shuffled version for later. Note that the version of <a href="../../Utils.html#Random.shuffle-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:AbstractArray"><code>shuffle</code></a> that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.</p><pre><code class="language-julia hljs">(xs,ys) = shuffle([x,y]);</code></pre><h2 id="Main-analysis"><a class="docs-heading-anchor" href="#Main-analysis">Main analysis</a><a id="Main-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Main-analysis" title="Permalink"></a></h2><p>We will try 3 BetaML models (<a href="../../Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}"><code>kmeans</code></a>, <a href="../../Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}"><code>kmedoids</code></a> and <a href="../../Clustering.html#BetaML.Clustering.gmm-Tuple{Any, Any}"><code>gmm</code></a>) and we compare them with <code>kmeans</code> from Clusterings.jl and <code>GMM</code> from GaussianMixtures.jl <code>Kmeans</code> and <code>kmedoids</code> works by first initialising the centers of the k-clusters (the &quot;representative&quot; (step a ) . For <code>kmeans</code> they must be selected within one of the data, for kmeans they are the geometrical center) n a nutshell. Then ( b ) iterate for each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( c ) move each representative at the center of its newly acquired cluster (where &quot;center&quot; depends again from the metric). Steps ( b ) and ( c ) are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don&#39;t move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes). The <code>gmm</code> algorithm is similar in that it employs an iterative approach (the Expectation<em>Minimisation algorithm, &quot;em&quot;) but here we make the hipothesis that the data points are the observed outcomes of some _mixture</em> probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables. Most <code>gmm</code> models use the Gaussain distribution as the family of the mixture components, so we can tought the <code>gmm</code> acronym to indicate <em>Gaussian Mixture Model</em>. In BetaML we do implemented only Gaussain components, but any distribution could be used by just subclassing <code>AbstractMixture</code> and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think &quot;gmm&quot; as an acronym for <em>Generative Mixture Model</em>. The algorithm try to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the &quot;E&quot; step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step <em>b</em> in the kmeans/kmedoids algorithm), and the &quot;M&quot; step estimates, giving the association probabilities in step &quot;M&quot;, the parameters of the mixture and of the individual components (similar to step <em>c</em>). The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as <code>gmm</code> can be used for many other things that clustering. It forms the backbone of the <a href="../../Clustering.html#BetaML.Clustering.predictMissing"><code>predictMissing</code></a> function to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, <code>predictMissing</code> can also be used to predict user&#39;s behaviours (or users&#39; appreciation) according to the behaviour/ranking made by pears (&quot;collaborative filtering&quot;). While the result of <code>gmm</code> is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call <code>mode()</code> to retrieve the class with the highest probability for each record.</p><p>As we are here, we also try different versions of the BetaML models, even if the default &quot;versions&quot; should be fine. For <code>kmeans</code> and <code>kmedoids</code> we will try different initialisation strategies (&quot;gird&quot;, the default one, &quot;random&quot; and &quot;shuffle&quot;), while for the <code>gmm</code> model we&#39;ll choose different distributions of the Gaussain family (<code>SphericalGaussian</code> - where the variance is a scalar, <code>DiagonalGaussian</code> - with a vector variance, and <code>FullGaussian</code>, where the covariance is a matrix).</p><p>As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the <a href="../../Utils.html#BetaML.Utils.crossValidation"><code>crossValidation</code></a> function. crossValidation works by calling the function <code>f</code>, defined by the user, passing to it the tuple <code>trainData</code>, <code>valData</code> and <code>rng</code> and collecting the result of the function f. The specific method for which <code>trainData</code>, and <code>valData</code> are selected at each iteration depends on the specific <code>sampler</code>. We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).</p><pre><code class="language-julia hljs">sampler = KFold(nSplits=5,nRepeats=3,shuffle=true, rng=copy(FIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">KFold(5, 3, true, StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7))</code></pre><p>We can now run the cross-validation with our models. Note that instead of defining the function <code>f</code> and then calling <code>crossValidation[f(trainData,testData,rng),[x,y],...)</code> we use the Julia <code>do</code> block syntax and we write directly the content of the <code>f</code> function in the <code>do</code> block. Also, by default crossValidation already returns the mean and the standard deviation of the output of the user-provided <code>f</code> function (or the <code>do</code> block). However this requires that the <code>f</code> function return a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with <code>returnStatistics=false</code> to crossValidation not to attempt to generate statistics but rather report the whole output. We&#39;ll compute the statistics ex-post.</p><p>Inside the <code>do</code> block we do 4 things:</p><ul><li>we recover from <code>trainData</code> (a tuple, as we passed a tuple to <code>crossValidation</code> too) the <code>xtrain</code> features and <code>ytrain</code> labels;</li><li>we run the various clustering algorithms</li><li>we use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T"><code>accuracy</code></a> has the parameter <code>ignoreLabels</code> to compute the accuracy oven any possible permutation of the classes found.</li><li>we return the various models&#39; accuracies</li></ul><pre><code class="language-julia hljs">cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng
          # For unsupervised learning we use only the train data.
          # Also, we use the associated labels only to measure the performances
         (xtrain,ytrain)  = trainData;
         # We run the clustering algorithm...
         clusteringOut     = kmeans(xtrain,3,rng=rng) ## init is grid by default
         # ... and we compute the accuracy using the real labels
         kMeansAccuracy    = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=&quot;random&quot;)
         kMeansRAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=&quot;shuffle&quot;)
         kMeansSAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng)   ## init is grid by default
         kMedoidsAccuracy  = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=&quot;random&quot;)
         kMedoidsRAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=&quot;shuffle&quot;)
         kMedoidsSAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = gmm(xtrain,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmSpherAccuracy  = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         clusteringOut     = gmm(xtrain,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmDiagAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         clusteringOut     = gmm(xtrain,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmFullAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         # For comparision with Clustering.jl
         clusteringOut     = Clustering.kmeans(xtrain&#39;, 3)
         kMeans2Accuracy   = accuracy(clusteringOut.assignments,ytrain,ignoreLabels=true)
         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException
         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)
         GaussianMixtures.em!(dGMM, xtrain)
         gmmDiag2Accuracy  = accuracy(GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ytrain,ignoreLabels=true)
         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)
         GaussianMixtures.em!(fGMM, xtrain)
         gmmFull2Accuracy  = accuracy(GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ytrain,ignoreLabels=true)
         # Returning the accuracies
         return kMeansAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy
 end

# We transform the output in matrix for easier analysis
accuracies = fill(0.0,(length(cOut),length(cOut[1])))
[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]
μs = mean(accuracies,dims=1)
σs = std(accuracies,dims=1)


modelLabels=[&quot;kMeansG&quot;,&quot;kMeansR&quot;,&quot;kMeansS&quot;,&quot;kMedoidsG&quot;,&quot;kMedoidsR&quot;,&quot;kMedoidsS&quot;,&quot;gmmSpher&quot;,&quot;gmmDiag&quot;,&quot;gmmFull&quot;,&quot;kMeans (Clustering.jl)&quot;,&quot;gmmDiag (GaussianMixtures.jl)&quot;,&quot;gmmFull (GaussianMixtures.jl)&quot;]
report = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs&#39;,digits=3),dims=2), stdAccuracy = dropdims(round.(σs&#39;,digits=3),dims=2))</code></pre><div class="data-frame"><p>12 rows × 3 columns</p><table class="data-frame"><thead><tr><th></th><th>mName</th><th>avgAccuracy</th><th>stdAccuracy</th></tr><tr><th></th><th title="String">String</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody><tr><th>1</th><td>kMeansG</td><td>0.891</td><td>0.017</td></tr><tr><th>2</th><td>kMeansR</td><td>0.866</td><td>0.083</td></tr><tr><th>3</th><td>kMeansS</td><td>0.764</td><td>0.174</td></tr><tr><th>4</th><td>kMedoidsG</td><td>0.894</td><td>0.015</td></tr><tr><th>5</th><td>kMedoidsR</td><td>0.804</td><td>0.144</td></tr><tr><th>6</th><td>kMedoidsS</td><td>0.893</td><td>0.018</td></tr><tr><th>7</th><td>gmmSpher</td><td>0.893</td><td>0.016</td></tr><tr><th>8</th><td>gmmDiag</td><td>0.917</td><td>0.022</td></tr><tr><th>9</th><td>gmmFull</td><td>0.97</td><td>0.035</td></tr><tr><th>10</th><td>kMeans (Clustering.jl)</td><td>0.858</td><td>0.11</td></tr><tr><th>11</th><td>gmmDiag (GaussianMixtures.jl)</td><td>0.882</td><td>0.096</td></tr><tr><th>12</th><td>gmmFull (GaussianMixtures.jl)</td><td>0.942</td><td>0.079</td></tr></tbody></table></div><h3 id="BetaML-model-accuracies"><a class="docs-heading-anchor" href="#BetaML-model-accuracies">BetaML model accuracies</a><a id="BetaML-model-accuracies-1"></a><a class="docs-heading-anchor-permalink" href="#BetaML-model-accuracies" title="Permalink"></a></h3><p>From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, also with very low variances. In detail, it is the (default) <code>grid</code> initialisation that leads to the better results for <code>kmeans</code> and <code>kmedoids</code>, while for the <code>gmm</code> models it is the <code>FullGaussian</code> to perform better.</p><h3 id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl"><a class="docs-heading-anchor" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl">Comparisions with <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code></a><a id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl" title="Permalink"></a></h3><p>For this specific case, both <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code> report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm &gt; Diagonal Gaussian &gt; Kmeans accuracy. I suspect the reason that BetaML gmm works so weel is in relation to the usage of kmeans algorithm with itself the grid initialisation. The grid initialisation &quot;guarantee&quot; indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.</p><h2 id="Working-without-the-labels"><a class="docs-heading-anchor" href="#Working-without-the-labels">Working without the labels</a><a id="Working-without-the-labels-1"></a><a class="docs-heading-anchor-permalink" href="#Working-without-the-labels" title="Permalink"></a></h2><p>Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don&#39;t have the true classes, or we wouln&#39;t need to do clustering in the first instance, so we don&#39;t know the number of classes to use. There are several methods to judge clusters algorithms goodness, perhaps the simplest one, at least for the expectation-maximisation algorithm employed in <code>gmm</code> to fit the data to the unknown mixture, is to use a information criteria that trade the goodness of the lickelyhood with the parameters used to do the fit. BetaML provide by default in the gmm clustering outputs both the <em>Bayesian information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}"><code>BIC</code></a>) and the <em>Akaike information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.aic-Tuple{Any, Any}"><code>AIC</code></a>), where for both a lower value is better.</p><p>We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence <code>crossValidation</code> again with the <code>FullGaussian</code> gmm model Note that we use the BIC/AIC criteria here for establishing the &quot;best&quot; number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail (but without using cross-validation) in the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>.</p><p>Let&#39;s try up to 4 possible classes:</p><pre><code class="language-julia hljs">K = 4
sampler = KFold(nSplits=5,nRepeats=2,shuffle=true, rng=copy(FIXEDRNG))
cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng
    (xtrain,ytrain)  = trainData;
    clusteringOut  = [gmm(xtrain,k,mixtures=[FullGaussian() for i in 1:k], verbosity=NONE, rng=rng) for k in 1:K]
    BICS           = [clusteringOut[i].BIC for i in 1:K]
    AICS           = [clusteringOut[i].AIC for i in 1:K]
    return (BICS,AICS)
end

# Transforming the output in matrices for easier analysis
Nit = length(cOut)

BICS = fill(0.0,(Nit,K))
AICS = fill(0.0,(Nit,K))
[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]
[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]

μsBICS = mean(BICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 762.112  516.031  539.392  593.272</code></pre><pre><code class="language-julia hljs">σsBICS = std(BICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026</code></pre><pre><code class="language-julia hljs">μsAICS = mean(AICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 723.087  435.194  416.743  428.81</code></pre><pre><code class="language-julia hljs">σsAICS = std(AICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026</code></pre><pre><code class="language-julia hljs">plot(1:K,[μsBICS&#39; μsAICS&#39;], labels=[&quot;BIC&quot; &quot;AIC&quot;], title=&quot;Information criteria by number of classes&quot;, xlabel=&quot;number of classes&quot;, ylabel=&quot;lower is better&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="150" height="100" viewBox="0 0 600 400">
<defs>
  <clipPath id="clip020">
    <rect x="0" y="0" width="600" height="400"/>
  </clipPath>
</defs>
<path clip-path="url(#clip020)" d="
M0 400 L600 400 L600 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip021">
    <rect x="120" y="0" width="421" height="400"/>
  </clipPath>
</defs>
<path clip-path="url(#clip020)" d="
M57.1193 355.795 L588.189 355.795 L588.189 30.868 L57.1193 30.868  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip022">
    <rect x="57" y="30" width="532" height="326"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  72.1496,355.795 72.1496,30.868 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  239.153,355.795 239.153,30.868 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  406.156,355.795 406.156,30.868 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  573.159,355.795 573.159,30.868 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,355.795 588.189,355.795 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  72.1496,355.795 72.1496,351.896 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  239.153,355.795 239.153,351.896 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  406.156,355.795 406.156,351.896 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  573.159,355.795 573.159,351.896 
  "/>
<path clip-path="url(#clip020)" d="M 0 0 M69.7451 369.979 L71.6548 369.979 L71.6548 363.388 L69.5773 363.804 L69.5773 362.74 L71.6432 362.323 L72.8122 362.323 L72.8122 369.979 L74.7219 369.979 L74.7219 370.963 L69.7451 370.963 L69.7451 369.979 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M237.816 369.979 L241.896 369.979 L241.896 370.963 L236.41 370.963 L236.41 369.979 Q237.075 369.291 238.221 368.133 Q239.373 366.97 239.668 366.634 Q240.229 366.003 240.449 365.569 Q240.675 365.13 240.675 364.707 Q240.675 364.019 240.189 363.585 Q239.708 363.151 238.933 363.151 Q238.383 363.151 237.77 363.341 Q237.162 363.532 236.467 363.92 L236.467 362.74 Q237.173 362.456 237.787 362.311 Q238.4 362.167 238.91 362.167 Q240.252 362.167 241.051 362.838 Q241.849 363.509 241.849 364.632 Q241.849 365.164 241.647 365.645 Q241.45 366.119 240.923 366.767 Q240.779 366.935 240.003 367.74 Q239.228 368.538 237.816 369.979 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M407.218 366.304 Q408.057 366.484 408.525 367.051 Q409 367.618 409 368.451 Q409 369.73 408.12 370.431 Q407.241 371.131 405.62 371.131 Q405.076 371.131 404.498 371.021 Q403.925 370.917 403.311 370.703 L403.311 369.574 Q403.797 369.858 404.376 370.002 Q404.955 370.147 405.586 370.147 Q406.685 370.147 407.258 369.713 Q407.837 369.279 407.837 368.451 Q407.837 367.688 407.299 367.259 Q406.766 366.825 405.811 366.825 L404.804 366.825 L404.804 365.865 L405.858 365.865 Q406.72 365.865 407.177 365.523 Q407.634 365.176 407.634 364.528 Q407.634 363.862 407.16 363.509 Q406.691 363.151 405.811 363.151 Q405.331 363.151 404.781 363.255 Q404.231 363.359 403.572 363.579 L403.572 362.537 Q404.237 362.352 404.816 362.259 Q405.4 362.167 405.916 362.167 Q407.247 362.167 408.022 362.774 Q408.797 363.376 408.797 364.406 Q408.797 365.124 408.387 365.622 Q407.976 366.113 407.218 366.304 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M573.911 363.341 L570.96 367.954 L573.911 367.954 L573.911 363.341 M573.604 362.323 L575.074 362.323 L575.074 367.954 L576.307 367.954 L576.307 368.926 L575.074 368.926 L575.074 370.963 L573.911 370.963 L573.911 368.926 L570.011 368.926 L570.011 367.797 L573.604 362.323 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M263.469 386.632 L263.469 392.011 L262.005 392.011 L262.005 386.68 Q262.005 385.414 261.512 384.786 Q261.018 384.157 260.032 384.157 Q258.846 384.157 258.162 384.913 Q257.477 385.669 257.477 386.974 L257.477 392.011 L256.005 392.011 L256.005 383.099 L257.477 383.099 L257.477 384.484 Q258.002 383.68 258.711 383.282 Q259.427 382.884 260.358 382.884 Q261.893 382.884 262.681 383.839 Q263.469 384.786 263.469 386.632 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M264.854 388.494 L264.854 383.099 L266.318 383.099 L266.318 388.438 Q266.318 389.703 266.811 390.34 Q267.304 390.969 268.291 390.969 Q269.477 390.969 270.161 390.213 Q270.853 389.457 270.853 388.152 L270.853 383.099 L272.317 383.099 L272.317 392.011 L270.853 392.011 L270.853 390.642 Q270.32 391.454 269.612 391.852 Q268.912 392.242 267.981 392.242 Q266.445 392.242 265.649 391.287 Q264.854 390.332 264.854 388.494 M268.538 382.884 L268.538 382.884 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M280.792 384.81 Q281.341 383.823 282.105 383.354 Q282.868 382.884 283.903 382.884 Q285.295 382.884 286.051 383.863 Q286.807 384.834 286.807 386.632 L286.807 392.011 L285.335 392.011 L285.335 386.68 Q285.335 385.399 284.882 384.778 Q284.428 384.157 283.497 384.157 Q282.359 384.157 281.699 384.913 Q281.038 385.669 281.038 386.974 L281.038 392.011 L279.566 392.011 L279.566 386.68 Q279.566 385.391 279.113 384.778 Q278.659 384.157 277.712 384.157 Q276.59 384.157 275.93 384.921 Q275.269 385.677 275.269 386.974 L275.269 392.011 L273.797 392.011 L273.797 383.099 L275.269 383.099 L275.269 384.484 Q275.771 383.664 276.471 383.274 Q277.171 382.884 278.134 382.884 Q279.105 382.884 279.781 383.377 Q280.465 383.871 280.792 384.81 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M294.741 387.563 Q294.741 385.948 294.072 385.033 Q293.412 384.11 292.25 384.11 Q291.088 384.11 290.42 385.033 Q289.759 385.948 289.759 387.563 Q289.759 389.178 290.42 390.101 Q291.088 391.016 292.25 391.016 Q293.412 391.016 294.072 390.101 Q294.741 389.178 294.741 387.563 M289.759 384.452 Q290.221 383.656 290.921 383.274 Q291.629 382.884 292.608 382.884 Q294.231 382.884 295.242 384.173 Q296.26 385.462 296.26 387.563 Q296.26 389.664 295.242 390.953 Q294.231 392.242 292.608 392.242 Q291.629 392.242 290.921 391.86 Q290.221 391.47 289.759 390.674 L289.759 392.011 L288.287 392.011 L288.287 379.63 L289.759 379.63 L289.759 384.452 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M305.419 387.189 L305.419 387.905 L298.687 387.905 Q298.783 389.417 299.594 390.213 Q300.414 391 301.87 391 Q302.714 391 303.501 390.794 Q304.297 390.587 305.077 390.173 L305.077 391.557 Q304.289 391.892 303.462 392.067 Q302.634 392.242 301.783 392.242 Q299.65 392.242 298.401 391 Q297.159 389.759 297.159 387.642 Q297.159 385.454 298.337 384.173 Q299.523 382.884 301.528 382.884 Q303.326 382.884 304.369 384.046 Q305.419 385.2 305.419 387.189 M303.955 386.759 Q303.939 385.558 303.279 384.842 Q302.626 384.125 301.544 384.125 Q300.318 384.125 299.578 384.818 Q298.846 385.51 298.735 386.767 L303.955 386.759 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M312.119 384.468 Q311.872 384.324 311.578 384.261 Q311.291 384.189 310.941 384.189 Q309.7 384.189 309.032 385.001 Q308.371 385.804 308.371 387.316 L308.371 392.011 L306.899 392.011 L306.899 383.099 L308.371 383.099 L308.371 384.484 Q308.833 383.672 309.573 383.282 Q310.313 382.884 311.371 382.884 Q311.522 382.884 311.705 382.908 Q311.888 382.924 312.111 382.964 L312.119 384.468 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M322.288 384.125 Q321.11 384.125 320.426 385.048 Q319.742 385.964 319.742 387.563 Q319.742 389.162 320.418 390.085 Q321.103 391 322.288 391 Q323.458 391 324.142 390.077 Q324.826 389.154 324.826 387.563 Q324.826 385.979 324.142 385.056 Q323.458 384.125 322.288 384.125 M322.288 382.884 Q324.198 382.884 325.288 384.125 Q326.378 385.367 326.378 387.563 Q326.378 389.751 325.288 391 Q324.198 392.242 322.288 392.242 Q320.37 392.242 319.28 391 Q318.198 389.751 318.198 387.563 Q318.198 385.367 319.28 384.125 Q320.37 382.884 322.288 382.884 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M332.426 379.63 L332.426 380.847 L331.025 380.847 Q330.237 380.847 329.927 381.165 Q329.625 381.484 329.625 382.311 L329.625 383.099 L332.036 383.099 L332.036 384.237 L329.625 384.237 L329.625 392.011 L328.153 392.011 L328.153 384.237 L326.752 384.237 L326.752 383.099 L328.153 383.099 L328.153 382.478 Q328.153 380.99 328.845 380.314 Q329.537 379.63 331.041 379.63 L332.426 379.63 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M345.555 383.441 L345.555 384.81 Q344.934 384.468 344.306 384.3 Q343.685 384.125 343.048 384.125 Q341.624 384.125 340.836 385.033 Q340.048 385.932 340.048 387.563 Q340.048 389.194 340.836 390.101 Q341.624 391 343.048 391 Q343.685 391 344.306 390.833 Q344.934 390.658 345.555 390.316 L345.555 391.669 Q344.942 391.955 344.282 392.098 Q343.629 392.242 342.889 392.242 Q340.876 392.242 339.69 390.977 Q338.505 389.711 338.505 387.563 Q338.505 385.383 339.698 384.133 Q340.9 382.884 342.985 382.884 Q343.661 382.884 344.306 383.027 Q344.95 383.163 345.555 383.441 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M347.091 379.63 L348.555 379.63 L348.555 392.011 L347.091 392.011 L347.091 379.63 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M354.141 387.531 Q352.366 387.531 351.682 387.937 Q350.997 388.343 350.997 389.321 Q350.997 390.101 351.507 390.563 Q352.024 391.016 352.907 391.016 Q354.125 391.016 354.857 390.157 Q355.597 389.29 355.597 387.857 L355.597 387.531 L354.141 387.531 M357.061 386.926 L357.061 392.011 L355.597 392.011 L355.597 390.658 Q355.095 391.47 354.347 391.86 Q353.599 392.242 352.517 392.242 Q351.149 392.242 350.337 391.478 Q349.533 390.706 349.533 389.417 Q349.533 387.913 350.536 387.149 Q351.547 386.385 353.544 386.385 L355.597 386.385 L355.597 386.242 Q355.597 385.231 354.928 384.682 Q354.268 384.125 353.066 384.125 Q352.302 384.125 351.578 384.308 Q350.854 384.491 350.186 384.857 L350.186 383.505 Q350.99 383.194 351.745 383.043 Q352.501 382.884 353.218 382.884 Q355.151 382.884 356.106 383.887 Q357.061 384.889 357.061 386.926 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M364.278 383.362 L364.278 384.746 Q363.657 384.428 362.989 384.269 Q362.32 384.11 361.604 384.11 Q360.514 384.11 359.965 384.444 Q359.424 384.778 359.424 385.446 Q359.424 385.956 359.814 386.25 Q360.204 386.536 361.382 386.799 L361.883 386.91 Q363.442 387.245 364.095 387.857 Q364.755 388.462 364.755 389.552 Q364.755 390.794 363.769 391.518 Q362.79 392.242 361.071 392.242 Q360.355 392.242 359.575 392.098 Q358.803 391.963 357.944 391.685 L357.944 390.173 Q358.756 390.595 359.543 390.809 Q360.331 391.016 361.103 391.016 Q362.137 391.016 362.694 390.666 Q363.251 390.308 363.251 389.664 Q363.251 389.067 362.846 388.749 Q362.448 388.43 361.087 388.136 L360.578 388.016 Q359.217 387.73 358.612 387.141 Q358.008 386.544 358.008 385.51 Q358.008 384.253 358.899 383.568 Q359.79 382.884 361.429 382.884 Q362.241 382.884 362.957 383.003 Q363.673 383.123 364.278 383.362 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M371.972 383.362 L371.972 384.746 Q371.352 384.428 370.683 384.269 Q370.015 384.11 369.299 384.11 Q368.209 384.11 367.66 384.444 Q367.119 384.778 367.119 385.446 Q367.119 385.956 367.509 386.25 Q367.898 386.536 369.076 386.799 L369.577 386.91 Q371.137 387.245 371.789 387.857 Q372.45 388.462 372.45 389.552 Q372.45 390.794 371.463 391.518 Q370.484 392.242 368.766 392.242 Q368.05 392.242 367.27 392.098 Q366.498 391.963 365.639 391.685 L365.639 390.173 Q366.45 390.595 367.238 390.809 Q368.026 391.016 368.798 391.016 Q369.832 391.016 370.389 390.666 Q370.946 390.308 370.946 389.664 Q370.946 389.067 370.54 388.749 Q370.142 388.43 368.782 388.136 L368.272 388.016 Q366.912 387.73 366.307 387.141 Q365.702 386.544 365.702 385.51 Q365.702 384.253 366.593 383.568 Q367.485 382.884 369.124 382.884 Q369.935 382.884 370.652 383.003 Q371.368 383.123 371.972 383.362 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M381.609 387.189 L381.609 387.905 L374.877 387.905 Q374.972 389.417 375.784 390.213 Q376.604 391 378.06 391 Q378.903 391 379.691 390.794 Q380.487 390.587 381.266 390.173 L381.266 391.557 Q380.479 391.892 379.651 392.067 Q378.824 392.242 377.972 392.242 Q375.84 392.242 374.59 391 Q373.349 389.759 373.349 387.642 Q373.349 385.454 374.527 384.173 Q375.712 382.884 377.718 382.884 Q379.516 382.884 380.558 384.046 Q381.609 385.2 381.609 387.189 M380.144 386.759 Q380.129 385.558 379.468 384.842 Q378.816 384.125 377.733 384.125 Q376.508 384.125 375.768 384.818 Q375.036 385.51 374.925 386.767 L380.144 386.759 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M388.826 383.362 L388.826 384.746 Q388.205 384.428 387.537 384.269 Q386.868 384.11 386.152 384.11 Q385.062 384.11 384.513 384.444 Q383.972 384.778 383.972 385.446 Q383.972 385.956 384.362 386.25 Q384.752 386.536 385.929 386.799 L386.431 386.91 Q387.99 387.245 388.643 387.857 Q389.303 388.462 389.303 389.552 Q389.303 390.794 388.316 391.518 Q387.338 392.242 385.619 392.242 Q384.903 392.242 384.123 392.098 Q383.351 391.963 382.492 391.685 L382.492 390.173 Q383.303 390.595 384.091 390.809 Q384.879 391.016 385.651 391.016 Q386.685 391.016 387.242 390.666 Q387.799 390.308 387.799 389.664 Q387.799 389.067 387.393 388.749 Q386.996 388.43 385.635 388.136 L385.126 388.016 Q383.765 387.73 383.16 387.141 Q382.555 386.544 382.555 385.51 Q382.555 384.253 383.447 383.568 Q384.338 382.884 385.977 382.884 Q386.789 382.884 387.505 383.003 Q388.221 383.123 388.826 383.362 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,317.081 588.189,317.081 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,272.703 588.189,272.703 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,228.326 588.189,228.326 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,183.948 588.189,183.948 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,139.57 588.189,139.57 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,95.1921 588.189,95.1921 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  57.1193,50.8143 588.189,50.8143 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,355.795 57.1193,30.868 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,317.081 63.4922,317.081 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,272.703 63.4922,272.703 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,228.326 63.4922,228.326 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,183.948 63.4922,183.948 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,139.57 63.4922,139.57 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,95.1921 63.4922,95.1921 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  57.1193,50.8143 63.4922,50.8143 
  "/>
<path clip-path="url(#clip020)" d="M 0 0 M32.4655 313.78 L29.5141 318.392 L32.4655 318.392 L32.4655 313.78 M32.1588 312.761 L33.6287 312.761 L33.6287 318.392 L34.8613 318.392 L34.8613 319.364 L33.6287 319.364 L33.6287 321.401 L32.4655 321.401 L32.4655 319.364 L28.565 319.364 L28.565 318.236 L32.1588 312.761 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M36.1402 312.761 L40.7293 312.761 L40.7293 313.745 L37.2108 313.745 L37.2108 315.863 Q37.4655 315.776 37.7201 315.736 Q37.9747 315.689 38.2293 315.689 Q39.6761 315.689 40.521 316.482 Q41.3659 317.275 41.3659 318.629 Q41.3659 320.024 40.4978 320.799 Q39.6298 321.569 38.0499 321.569 Q37.506 321.569 36.9388 321.476 Q36.3775 321.384 35.7756 321.199 L35.7756 320.024 Q36.2965 320.307 36.852 320.446 Q37.4076 320.585 38.0268 320.585 Q39.0279 320.585 39.6124 320.059 Q40.1969 319.532 40.1969 318.629 Q40.1969 317.726 39.6124 317.2 Q39.0279 316.673 38.0268 316.673 Q37.558 316.673 37.0893 316.777 Q36.6263 316.882 36.1402 317.101 L36.1402 312.761 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 313.531 Q44.2305 313.531 43.7733 314.422 Q43.3219 315.307 43.3219 317.09 Q43.3219 318.866 43.7733 319.758 Q44.2305 320.643 45.1332 320.643 Q46.0418 320.643 46.4932 319.758 Q46.9504 318.866 46.9504 317.09 Q46.9504 315.307 46.4932 314.422 Q46.0418 313.531 45.1332 313.531 M45.1332 312.605 Q46.5858 312.605 47.3497 313.757 Q48.1193 314.902 48.1193 317.09 Q48.1193 319.272 47.3497 320.423 Q46.5858 321.569 45.1332 321.569 Q43.6807 321.569 42.911 320.423 Q42.1471 319.272 42.1471 317.09 Q42.1471 314.902 42.911 313.757 Q43.6807 312.605 45.1332 312.605 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M29.3868 268.383 L33.9759 268.383 L33.9759 269.367 L30.4574 269.367 L30.4574 271.485 Q30.712 271.398 30.9666 271.358 Q31.2213 271.312 31.4759 271.312 Q32.9227 271.312 33.7676 272.104 Q34.6125 272.897 34.6125 274.251 Q34.6125 275.646 33.7444 276.422 Q32.8764 277.191 31.2965 277.191 Q30.7525 277.191 30.1854 277.099 Q29.6241 277.006 29.0222 276.821 L29.0222 275.646 Q29.543 275.93 30.0986 276.069 Q30.6541 276.207 31.2734 276.207 Q32.2745 276.207 32.859 275.681 Q33.4435 275.154 33.4435 274.251 Q33.4435 273.349 32.859 272.822 Q32.2745 272.295 31.2734 272.295 Q30.8046 272.295 30.3359 272.4 Q29.8729 272.504 29.3868 272.724 L29.3868 268.383 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M38.3798 269.153 Q37.477 269.153 37.0199 270.044 Q36.5685 270.93 36.5685 272.712 Q36.5685 274.489 37.0199 275.38 Q37.477 276.265 38.3798 276.265 Q39.2884 276.265 39.7397 275.38 Q40.1969 274.489 40.1969 272.712 Q40.1969 270.93 39.7397 270.044 Q39.2884 269.153 38.3798 269.153 M38.3798 268.227 Q39.8323 268.227 40.5962 269.379 Q41.3659 270.525 41.3659 272.712 Q41.3659 274.894 40.5962 276.045 Q39.8323 277.191 38.3798 277.191 Q36.9273 277.191 36.1576 276.045 Q35.3937 274.894 35.3937 272.712 Q35.3937 270.525 36.1576 269.379 Q36.9273 268.227 38.3798 268.227 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 269.153 Q44.2305 269.153 43.7733 270.044 Q43.3219 270.93 43.3219 272.712 Q43.3219 274.489 43.7733 275.38 Q44.2305 276.265 45.1332 276.265 Q46.0418 276.265 46.4932 275.38 Q46.9504 274.489 46.9504 272.712 Q46.9504 270.93 46.4932 270.044 Q46.0418 269.153 45.1332 269.153 M45.1332 268.227 Q46.5858 268.227 47.3497 269.379 Q48.1193 270.525 48.1193 272.712 Q48.1193 274.894 47.3497 276.045 Q46.5858 277.191 45.1332 277.191 Q43.6807 277.191 42.911 276.045 Q42.1471 274.894 42.1471 272.712 Q42.1471 270.525 42.911 269.379 Q43.6807 268.227 45.1332 268.227 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M29.6356 224.006 L34.2247 224.006 L34.2247 224.989 L30.7062 224.989 L30.7062 227.107 Q30.9609 227.021 31.2155 226.98 Q31.4701 226.934 31.7247 226.934 Q33.1715 226.934 34.0164 227.727 Q34.8613 228.519 34.8613 229.874 Q34.8613 231.268 33.9933 232.044 Q33.1252 232.813 31.5453 232.813 Q31.0014 232.813 30.4342 232.721 Q29.8729 232.628 29.2711 232.443 L29.2711 231.268 Q29.7919 231.552 30.3474 231.691 Q30.903 231.83 31.5222 231.83 Q32.5234 231.83 33.1078 231.303 Q33.6923 230.776 33.6923 229.874 Q33.6923 228.971 33.1078 228.444 Q32.5234 227.918 31.5222 227.918 Q31.0535 227.918 30.5847 228.022 Q30.1217 228.126 29.6356 228.346 L29.6356 224.006 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M36.1402 224.006 L40.7293 224.006 L40.7293 224.989 L37.2108 224.989 L37.2108 227.107 Q37.4655 227.021 37.7201 226.98 Q37.9747 226.934 38.2293 226.934 Q39.6761 226.934 40.521 227.727 Q41.3659 228.519 41.3659 229.874 Q41.3659 231.268 40.4978 232.044 Q39.6298 232.813 38.0499 232.813 Q37.506 232.813 36.9388 232.721 Q36.3775 232.628 35.7756 232.443 L35.7756 231.268 Q36.2965 231.552 36.852 231.691 Q37.4076 231.83 38.0268 231.83 Q39.0279 231.83 39.6124 231.303 Q40.1969 230.776 40.1969 229.874 Q40.1969 228.971 39.6124 228.444 Q39.0279 227.918 38.0268 227.918 Q37.558 227.918 37.0893 228.022 Q36.6263 228.126 36.1402 228.346 L36.1402 224.006 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 224.775 Q44.2305 224.775 43.7733 225.666 Q43.3219 226.552 43.3219 228.334 Q43.3219 230.111 43.7733 231.002 Q44.2305 231.887 45.1332 231.887 Q46.0418 231.887 46.4932 231.002 Q46.9504 230.111 46.9504 228.334 Q46.9504 226.552 46.4932 225.666 Q46.0418 224.775 45.1332 224.775 M45.1332 223.849 Q46.5858 223.849 47.3497 225.001 Q48.1193 226.147 48.1193 228.334 Q48.1193 230.516 47.3497 231.668 Q46.5858 232.813 45.1332 232.813 Q43.6807 232.813 42.911 231.668 Q42.1471 230.516 42.1471 228.334 Q42.1471 226.147 42.911 225.001 Q43.6807 223.849 45.1332 223.849 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M31.7305 183.482 Q30.9435 183.482 30.4805 184.02 Q30.0234 184.558 30.0234 185.496 Q30.0234 186.427 30.4805 186.971 Q30.9435 187.51 31.7305 187.51 Q32.5176 187.51 32.9747 186.971 Q33.4377 186.427 33.4377 185.496 Q33.4377 184.558 32.9747 184.02 Q32.5176 183.482 31.7305 183.482 M34.0511 179.819 L34.0511 180.884 Q33.6113 180.675 33.1599 180.565 Q32.7143 180.455 32.2745 180.455 Q31.1171 180.455 30.5037 181.237 Q29.8961 182.018 29.8092 183.598 Q30.1507 183.094 30.6657 182.828 Q31.1808 182.556 31.8 182.556 Q33.1021 182.556 33.8544 183.349 Q34.6125 184.136 34.6125 185.496 Q34.6125 186.827 33.8254 187.631 Q33.0384 188.436 31.7305 188.436 Q30.2317 188.436 29.4389 187.29 Q28.6461 186.138 28.6461 183.956 Q28.6461 181.908 29.6183 180.693 Q30.5905 179.472 32.2282 179.472 Q32.668 179.472 33.1136 179.558 Q33.565 179.645 34.0511 179.819 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M38.3798 180.397 Q37.477 180.397 37.0199 181.289 Q36.5685 182.174 36.5685 183.956 Q36.5685 185.733 37.0199 186.624 Q37.477 187.51 38.3798 187.51 Q39.2884 187.51 39.7397 186.624 Q40.1969 185.733 40.1969 183.956 Q40.1969 182.174 39.7397 181.289 Q39.2884 180.397 38.3798 180.397 M38.3798 179.472 Q39.8323 179.472 40.5962 180.623 Q41.3659 181.769 41.3659 183.956 Q41.3659 186.138 40.5962 187.29 Q39.8323 188.436 38.3798 188.436 Q36.9273 188.436 36.1576 187.29 Q35.3937 186.138 35.3937 183.956 Q35.3937 181.769 36.1576 180.623 Q36.9273 179.472 38.3798 179.472 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 180.397 Q44.2305 180.397 43.7733 181.289 Q43.3219 182.174 43.3219 183.956 Q43.3219 185.733 43.7733 186.624 Q44.2305 187.51 45.1332 187.51 Q46.0418 187.51 46.4932 186.624 Q46.9504 185.733 46.9504 183.956 Q46.9504 182.174 46.4932 181.289 Q46.0418 180.397 45.1332 180.397 M45.1332 179.472 Q46.5858 179.472 47.3497 180.623 Q48.1193 181.769 48.1193 183.956 Q48.1193 186.138 47.3497 187.29 Q46.5858 188.436 45.1332 188.436 Q43.6807 188.436 42.911 187.29 Q42.1471 186.138 42.1471 183.956 Q42.1471 181.769 42.911 180.623 Q43.6807 179.472 45.1332 179.472 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M31.9794 139.104 Q31.1923 139.104 30.7294 139.642 Q30.2722 140.18 30.2722 141.118 Q30.2722 142.05 30.7294 142.594 Q31.1923 143.132 31.9794 143.132 Q32.7664 143.132 33.2236 142.594 Q33.6865 142.05 33.6865 141.118 Q33.6865 140.18 33.2236 139.642 Q32.7664 139.104 31.9794 139.104 M34.3 135.441 L34.3 136.506 Q33.8601 136.297 33.4088 136.187 Q32.9632 136.077 32.5234 136.077 Q31.366 136.077 30.7525 136.859 Q30.1449 137.64 30.0581 139.22 Q30.3995 138.716 30.9146 138.45 Q31.4296 138.178 32.0488 138.178 Q33.3509 138.178 34.1032 138.971 Q34.8613 139.758 34.8613 141.118 Q34.8613 142.449 34.0743 143.253 Q33.2872 144.058 31.9794 144.058 Q30.4805 144.058 29.6877 142.912 Q28.8949 141.76 28.8949 139.579 Q28.8949 137.53 29.8671 136.315 Q30.8393 135.094 32.4771 135.094 Q32.9169 135.094 33.3625 135.18 Q33.8139 135.267 34.3 135.441 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M36.1402 135.25 L40.7293 135.25 L40.7293 136.234 L37.2108 136.234 L37.2108 138.352 Q37.4655 138.265 37.7201 138.224 Q37.9747 138.178 38.2293 138.178 Q39.6761 138.178 40.521 138.971 Q41.3659 139.764 41.3659 141.118 Q41.3659 142.513 40.4978 143.288 Q39.6298 144.058 38.0499 144.058 Q37.506 144.058 36.9388 143.965 Q36.3775 143.873 35.7756 143.687 L35.7756 142.513 Q36.2965 142.796 36.852 142.935 Q37.4076 143.074 38.0268 143.074 Q39.0279 143.074 39.6124 142.547 Q40.1969 142.021 40.1969 141.118 Q40.1969 140.215 39.6124 139.689 Q39.0279 139.162 38.0268 139.162 Q37.558 139.162 37.0893 139.266 Q36.6263 139.37 36.1402 139.59 L36.1402 135.25 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 136.02 Q44.2305 136.02 43.7733 136.911 Q43.3219 137.796 43.3219 139.579 Q43.3219 141.355 43.7733 142.246 Q44.2305 143.132 45.1332 143.132 Q46.0418 143.132 46.4932 142.246 Q46.9504 141.355 46.9504 139.579 Q46.9504 137.796 46.4932 136.911 Q46.0418 136.02 45.1332 136.02 M45.1332 135.094 Q46.5858 135.094 47.3497 136.245 Q48.1193 137.391 48.1193 139.579 Q48.1193 141.76 47.3497 142.912 Q46.5858 144.058 45.1332 144.058 Q43.6807 144.058 42.911 142.912 Q42.1471 141.76 42.1471 139.579 Q42.1471 137.391 42.911 136.245 Q43.6807 135.094 45.1332 135.094 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M29.0569 90.8721 L34.6125 90.8721 L34.6125 91.3698 L31.4759 99.5121 L30.2548 99.5121 L33.2062 91.8559 L29.0569 91.8559 L29.0569 90.8721 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M38.3798 91.6418 Q37.477 91.6418 37.0199 92.533 Q36.5685 93.4184 36.5685 95.2008 Q36.5685 96.9774 37.0199 97.8686 Q37.477 98.754 38.3798 98.754 Q39.2884 98.754 39.7397 97.8686 Q40.1969 96.9774 40.1969 95.2008 Q40.1969 93.4184 39.7397 92.533 Q39.2884 91.6418 38.3798 91.6418 M38.3798 90.7159 Q39.8323 90.7159 40.5962 91.8675 Q41.3659 93.0133 41.3659 95.2008 Q41.3659 97.3825 40.5962 98.5341 Q39.8323 99.6799 38.3798 99.6799 Q36.9273 99.6799 36.1576 98.5341 Q35.3937 97.3825 35.3937 95.2008 Q35.3937 93.0133 36.1576 91.8675 Q36.9273 90.7159 38.3798 90.7159 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 91.6418 Q44.2305 91.6418 43.7733 92.533 Q43.3219 93.4184 43.3219 95.2008 Q43.3219 96.9774 43.7733 97.8686 Q44.2305 98.754 45.1332 98.754 Q46.0418 98.754 46.4932 97.8686 Q46.9504 96.9774 46.9504 95.2008 Q46.9504 93.4184 46.4932 92.533 Q46.0418 91.6418 45.1332 91.6418 M45.1332 90.7159 Q46.5858 90.7159 47.3497 91.8675 Q48.1193 93.0133 48.1193 95.2008 Q48.1193 97.3825 47.3497 98.5341 Q46.5858 99.6799 45.1332 99.6799 Q43.6807 99.6799 42.911 98.5341 Q42.1471 97.3825 42.1471 95.2008 Q42.1471 93.0133 42.911 91.8675 Q43.6807 90.7159 45.1332 90.7159 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M29.3058 46.4943 L34.8613 46.4943 L34.8613 46.992 L31.7247 55.1343 L30.5037 55.1343 L33.4551 47.4781 L29.3058 47.4781 L29.3058 46.4943 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M36.1402 46.4943 L40.7293 46.4943 L40.7293 47.4781 L37.2108 47.4781 L37.2108 49.5961 Q37.4655 49.5093 37.7201 49.4688 Q37.9747 49.4225 38.2293 49.4225 Q39.6761 49.4225 40.521 50.2153 Q41.3659 51.0082 41.3659 52.3623 Q41.3659 53.757 40.4978 54.5325 Q39.6298 55.3021 38.0499 55.3021 Q37.506 55.3021 36.9388 55.2095 Q36.3775 55.1169 35.7756 54.9318 L35.7756 53.757 Q36.2965 54.0406 36.852 54.1794 Q37.4076 54.3183 38.0268 54.3183 Q39.0279 54.3183 39.6124 53.7917 Q40.1969 53.2651 40.1969 52.3623 Q40.1969 51.4596 39.6124 50.9329 Q39.0279 50.4063 38.0268 50.4063 Q37.558 50.4063 37.0893 50.5105 Q36.6263 50.6146 36.1402 50.8346 L36.1402 46.4943 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M45.1332 47.264 Q44.2305 47.264 43.7733 48.1552 Q43.3219 49.0406 43.3219 50.823 Q43.3219 52.5996 43.7733 53.4908 Q44.2305 54.3762 45.1332 54.3762 Q46.0418 54.3762 46.4932 53.4908 Q46.9504 52.5996 46.9504 50.823 Q46.9504 49.0406 46.4932 48.1552 Q46.0418 47.264 45.1332 47.264 M45.1332 46.3381 Q46.5858 46.3381 47.3497 47.4897 Q48.1193 48.6355 48.1193 50.823 Q48.1193 53.0047 47.3497 54.1563 Q46.5858 55.3021 45.1332 55.3021 Q43.6807 55.3021 42.911 54.1563 Q42.1471 53.0047 42.1471 50.823 Q42.1471 48.6355 42.911 47.4897 Q43.6807 46.3381 45.1332 46.3381 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M3.61974 246.616 L3.61974 245.152 L16.001 245.152 L16.001 246.616 L3.61974 246.616 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M8.11552 240.163 Q8.11552 241.341 9.03855 242.025 Q9.95362 242.709 11.553 242.709 Q13.1524 242.709 14.0754 242.033 Q14.9905 241.349 14.9905 240.163 Q14.9905 238.994 14.0675 238.309 Q13.1444 237.625 11.553 237.625 Q9.96953 237.625 9.0465 238.309 Q8.11552 238.994 8.11552 240.163 M6.87421 240.163 Q6.87421 238.253 8.11552 237.163 Q9.35683 236.073 11.553 236.073 Q13.7412 236.073 14.9905 237.163 Q16.2318 238.253 16.2318 240.163 Q16.2318 242.081 14.9905 243.171 Q13.7412 244.253 11.553 244.253 Q9.35683 244.253 8.11552 243.171 Q6.87421 242.081 6.87421 240.163 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M7.08905 235.389 L7.08905 233.925 L14.0436 232.095 L7.08905 230.272 L7.08905 228.546 L14.0436 226.716 L7.08905 224.893 L7.08905 223.429 L16.001 225.761 L16.001 227.487 L8.69639 229.405 L16.001 231.331 L16.001 233.057 L7.08905 235.389 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M11.179 214.271 L11.8952 214.271 L11.8952 221.002 Q13.407 220.907 14.2027 220.095 Q14.9905 219.276 14.9905 217.82 Q14.9905 216.976 14.7836 216.188 Q14.5767 215.393 14.1629 214.613 L15.5475 214.613 Q15.8817 215.401 16.0567 216.228 Q16.2318 217.056 16.2318 217.907 Q16.2318 220.04 14.9905 221.289 Q13.7492 222.53 11.6326 222.53 Q9.44436 222.53 8.16326 221.353 Q6.87421 220.167 6.87421 218.162 Q6.87421 216.363 8.03595 215.321 Q9.18973 214.271 11.179 214.271 M10.7493 215.735 Q9.5478 215.751 8.83166 216.411 Q8.11552 217.064 8.11552 218.146 Q8.11552 219.371 8.80779 220.111 Q9.50006 220.843 10.7573 220.955 L10.7493 215.735 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M8.45768 207.571 Q8.31445 207.817 8.25079 208.112 Q8.17918 208.398 8.17918 208.748 Q8.17918 209.99 8.9908 210.658 Q9.79448 211.319 11.3063 211.319 L16.001 211.319 L16.001 212.791 L7.08905 212.791 L7.08905 211.319 L8.47359 211.319 Q7.66196 210.857 7.27206 210.117 Q6.87421 209.377 6.87421 208.319 Q6.87421 208.168 6.89808 207.985 Q6.91399 207.802 6.95378 207.579 L8.45768 207.571 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M7.08905 200.855 L7.08905 199.391 L16.001 199.391 L16.001 200.855 L7.08905 200.855 M3.61974 200.855 L3.61974 199.391 L5.47375 199.391 L5.47375 200.855 L3.61974 200.855 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M7.35164 192.174 L8.73618 192.174 Q8.41789 192.794 8.25875 193.463 Q8.09961 194.131 8.09961 194.847 Q8.09961 195.937 8.43381 196.486 Q8.76801 197.028 9.4364 197.028 Q9.94566 197.028 10.2401 196.638 Q10.5265 196.248 10.7891 195.07 L10.9005 194.569 Q11.2347 193.009 11.8474 192.357 Q12.4522 191.696 13.5423 191.696 Q14.7836 191.696 15.5077 192.683 Q16.2318 193.662 16.2318 195.38 Q16.2318 196.097 16.0886 196.876 Q15.9533 197.648 15.6748 198.508 L14.1629 198.508 Q14.5847 197.696 14.7995 196.908 Q15.0064 196.12 15.0064 195.349 Q15.0064 194.314 14.6563 193.757 Q14.2982 193.2 13.6537 193.2 Q13.0569 193.2 12.7386 193.606 Q12.4203 194.004 12.1259 195.365 L12.0066 195.874 Q11.7201 197.234 11.1313 197.839 Q10.5345 198.444 9.50006 198.444 Q8.24283 198.444 7.55852 197.553 Q6.87421 196.662 6.87421 195.022 Q6.87421 194.211 6.99356 193.495 Q7.11292 192.778 7.35164 192.174 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M11.553 178.583 Q9.9377 178.583 9.02263 179.251 Q8.09961 179.912 8.09961 181.074 Q8.09961 182.235 9.02263 182.904 Q9.9377 183.564 11.553 183.564 Q13.1683 183.564 14.0913 182.904 Q15.0064 182.235 15.0064 181.074 Q15.0064 179.912 14.0913 179.251 Q13.1683 178.583 11.553 178.583 M8.44176 183.564 Q7.64605 183.103 7.26411 182.402 Q6.87421 181.694 6.87421 180.715 Q6.87421 179.092 8.16326 178.082 Q9.45232 177.063 11.553 177.063 Q13.6537 177.063 14.9427 178.082 Q16.2318 179.092 16.2318 180.715 Q16.2318 181.694 15.8499 182.402 Q15.46 183.103 14.6642 183.564 L16.001 183.564 L16.001 185.036 L3.61974 185.036 L3.61974 183.564 L8.44176 183.564 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M11.179 167.904 L11.8952 167.904 L11.8952 174.636 Q13.407 174.541 14.2027 173.729 Q14.9905 172.91 14.9905 171.453 Q14.9905 170.61 14.7836 169.822 Q14.5767 169.026 14.1629 168.247 L15.5475 168.247 Q15.8817 169.034 16.0567 169.862 Q16.2318 170.689 16.2318 171.541 Q16.2318 173.673 14.9905 174.923 Q13.7492 176.164 11.6326 176.164 Q9.44436 176.164 8.16326 174.986 Q6.87421 173.801 6.87421 171.796 Q6.87421 169.997 8.03595 168.955 Q9.18973 167.904 11.179 167.904 M10.7493 169.369 Q9.5478 169.385 8.83166 170.045 Q8.11552 170.697 8.11552 171.78 Q8.11552 173.005 8.80779 173.745 Q9.50006 174.477 10.7573 174.588 L10.7493 169.369 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M4.55868 164.921 L7.08905 164.921 L7.08905 161.905 L8.22692 161.905 L8.22692 164.921 L13.0649 164.921 Q14.155 164.921 14.4653 164.626 Q14.7756 164.324 14.7756 163.409 L14.7756 161.905 L16.001 161.905 L16.001 163.409 Q16.001 165.104 15.3724 165.748 Q14.7359 166.393 13.0649 166.393 L8.22692 166.393 L8.22692 167.467 L7.08905 167.467 L7.08905 166.393 L4.55868 166.393 L4.55868 164.921 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M4.55868 158.921 L7.08905 158.921 L7.08905 155.905 L8.22692 155.905 L8.22692 158.921 L13.0649 158.921 Q14.155 158.921 14.4653 158.626 Q14.7756 158.324 14.7756 157.409 L14.7756 155.905 L16.001 155.905 L16.001 157.409 Q16.001 159.104 15.3724 159.748 Q14.7359 160.393 13.0649 160.393 L8.22692 160.393 L8.22692 161.467 L7.08905 161.467 L7.08905 160.393 L4.55868 160.393 L4.55868 158.921 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M11.179 146.746 L11.8952 146.746 L11.8952 153.478 Q13.407 153.383 14.2027 152.571 Q14.9905 151.751 14.9905 150.295 Q14.9905 149.452 14.7836 148.664 Q14.5767 147.868 14.1629 147.089 L15.5475 147.089 Q15.8817 147.876 16.0567 148.704 Q16.2318 149.531 16.2318 150.383 Q16.2318 152.515 14.9905 153.765 Q13.7492 155.006 11.6326 155.006 Q9.44436 155.006 8.16326 153.828 Q6.87421 152.643 6.87421 150.637 Q6.87421 148.839 8.03595 147.797 Q9.18973 146.746 11.179 146.746 M10.7493 148.211 Q9.5478 148.226 8.83166 148.887 Q8.11552 149.539 8.11552 150.622 Q8.11552 151.847 8.80779 152.587 Q9.50006 153.319 10.7573 153.43 L10.7493 148.211 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M8.45768 140.047 Q8.31445 140.293 8.25079 140.588 Q8.17918 140.874 8.17918 141.224 Q8.17918 142.466 8.9908 143.134 Q9.79448 143.794 11.3063 143.794 L16.001 143.794 L16.001 145.266 L7.08905 145.266 L7.08905 143.794 L8.47359 143.794 Q7.66196 143.333 7.27206 142.593 Q6.87421 141.853 6.87421 140.795 Q6.87421 140.643 6.89808 140.46 Q6.91399 140.277 6.95378 140.055 L8.45768 140.047 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M130.439 3.024 L132.484 3.024 L132.484 18.144 L130.439 18.144 L130.439 3.024 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M143.867 11.298 L143.867 18.144 L142.004 18.144 L142.004 11.3587 Q142.004 9.7485 141.376 8.94845 Q140.748 8.14839 139.493 8.14839 Q137.984 8.14839 137.113 9.11048 Q136.242 10.0726 136.242 11.7334 L136.242 18.144 L134.368 18.144 L134.368 6.80147 L136.242 6.80147 L136.242 8.56361 Q136.91 7.54076 137.811 7.0344 Q138.723 6.52803 139.908 6.52803 Q141.862 6.52803 142.865 7.7433 Q143.867 8.94845 143.867 11.298 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M151.564 2.38598 L151.564 3.93545 L149.782 3.93545 Q148.779 3.93545 148.384 4.34054 Q147.999 4.74563 147.999 5.79887 L147.999 6.80147 L151.068 6.80147 L151.068 8.24967 L147.999 8.24967 L147.999 18.144 L146.126 18.144 L146.126 8.24967 L144.343 8.24967 L144.343 6.80147 L146.126 6.80147 L146.126 6.01154 Q146.126 4.11774 147.007 3.25693 Q147.888 2.38598 149.802 2.38598 L151.564 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M157.914 8.10788 Q156.415 8.10788 155.544 9.28265 Q154.673 10.4473 154.673 12.4829 Q154.673 14.5184 155.534 15.6932 Q156.405 16.8578 157.914 16.8578 Q159.403 16.8578 160.274 15.6831 Q161.145 14.5083 161.145 12.4829 Q161.145 10.4675 160.274 9.29277 Q159.403 8.10788 157.914 8.10788 M157.914 6.52803 Q160.345 6.52803 161.732 8.10788 Q163.119 9.68774 163.119 12.4829 Q163.119 15.2679 161.732 16.8578 Q160.345 18.4377 157.914 18.4377 Q155.473 18.4377 154.086 16.8578 Q152.709 15.2679 152.709 12.4829 Q152.709 9.68774 154.086 8.10788 Q155.473 6.52803 157.914 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M171.647 8.54336 Q171.333 8.36107 170.958 8.28005 Q170.593 8.1889 170.148 8.1889 Q168.568 8.1889 167.717 9.22188 Q166.877 10.2447 166.877 12.1689 L166.877 18.144 L165.003 18.144 L165.003 6.80147 L166.877 6.80147 L166.877 8.56361 Q167.464 7.53063 168.406 7.0344 Q169.348 6.52803 170.695 6.52803 Q170.887 6.52803 171.12 6.55841 Q171.353 6.57867 171.636 6.6293 L171.647 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M182.068 8.97883 Q182.766 7.72305 183.739 7.12554 Q184.711 6.52803 186.027 6.52803 Q187.8 6.52803 188.762 7.77369 Q189.724 9.00921 189.724 11.298 L189.724 18.144 L187.85 18.144 L187.85 11.3587 Q187.85 9.72825 187.273 8.93832 Q186.696 8.14839 185.511 8.14839 Q184.063 8.14839 183.222 9.11048 Q182.381 10.0726 182.381 11.7334 L182.381 18.144 L180.508 18.144 L180.508 11.3587 Q180.508 9.71812 179.931 8.93832 Q179.353 8.14839 178.148 8.14839 Q176.72 8.14839 175.88 9.12061 Q175.039 10.0827 175.039 11.7334 L175.039 18.144 L173.166 18.144 L173.166 6.80147 L175.039 6.80147 L175.039 8.56361 Q175.677 7.5205 176.568 7.02427 Q177.46 6.52803 178.685 6.52803 Q179.921 6.52803 180.781 7.15592 Q181.652 7.78381 182.068 8.97883 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M196.833 12.4424 Q194.575 12.4424 193.704 12.9588 Q192.833 13.4753 192.833 14.721 Q192.833 15.7135 193.481 16.3008 Q194.139 16.8781 195.263 16.8781 Q196.813 16.8781 197.745 15.7843 Q198.686 14.6805 198.686 12.8576 L198.686 12.4424 L196.833 12.4424 M200.55 11.6727 L200.55 18.144 L198.686 18.144 L198.686 16.4224 Q198.048 17.4553 197.096 17.9516 Q196.144 18.4377 194.767 18.4377 Q193.025 18.4377 191.992 17.4655 Q190.969 16.4831 190.969 14.8425 Q190.969 12.9285 192.245 11.9562 Q193.532 10.984 196.074 10.984 L198.686 10.984 L198.686 10.8017 Q198.686 9.51557 197.836 8.81679 Q196.995 8.10788 195.466 8.10788 Q194.494 8.10788 193.572 8.34081 Q192.651 8.57374 191.8 9.03959 L191.8 7.31796 Q192.823 6.923 193.785 6.73058 Q194.747 6.52803 195.658 6.52803 Q198.119 6.52803 199.335 7.80407 Q200.55 9.0801 200.55 11.6727 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M204.347 3.581 L204.347 6.80147 L208.186 6.80147 L208.186 8.24967 L204.347 8.24967 L204.347 14.407 Q204.347 15.7945 204.722 16.1894 Q205.107 16.5844 206.272 16.5844 L208.186 16.5844 L208.186 18.144 L206.272 18.144 Q204.115 18.144 203.294 17.3439 Q202.474 16.5338 202.474 14.407 L202.474 8.24967 L201.107 8.24967 L201.107 6.80147 L202.474 6.80147 L202.474 3.581 L204.347 3.581 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M210.14 6.80147 L212.004 6.80147 L212.004 18.144 L210.14 18.144 L210.14 6.80147 M210.14 2.38598 L212.004 2.38598 L212.004 4.74563 L210.14 4.74563 L210.14 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M218.353 8.10788 Q216.855 8.10788 215.984 9.28265 Q215.113 10.4473 215.113 12.4829 Q215.113 14.5184 215.974 15.6932 Q216.845 16.8578 218.353 16.8578 Q219.842 16.8578 220.713 15.6831 Q221.584 14.5083 221.584 12.4829 Q221.584 10.4675 220.713 9.29277 Q219.842 8.10788 218.353 8.10788 M218.353 6.52803 Q220.784 6.52803 222.171 8.10788 Q223.559 9.68774 223.559 12.4829 Q223.559 15.2679 222.171 16.8578 Q220.784 18.4377 218.353 18.4377 Q215.913 18.4377 214.525 16.8578 Q213.148 15.2679 213.148 12.4829 Q213.148 9.68774 214.525 8.10788 Q215.913 6.52803 218.353 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M234.942 11.298 L234.942 18.144 L233.079 18.144 L233.079 11.3587 Q233.079 9.7485 232.451 8.94845 Q231.823 8.14839 230.567 8.14839 Q229.058 8.14839 228.187 9.11048 Q227.316 10.0726 227.316 11.7334 L227.316 18.144 L225.443 18.144 L225.443 6.80147 L227.316 6.80147 L227.316 8.56361 Q227.985 7.54076 228.886 7.0344 Q229.797 6.52803 230.982 6.52803 Q232.937 6.52803 233.939 7.7433 Q234.942 8.94845 234.942 11.298 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M251.652 7.23694 L251.652 8.97883 Q250.862 8.54336 250.062 8.33068 Q249.272 8.10788 248.462 8.10788 Q246.649 8.10788 245.646 9.26239 Q244.644 10.4068 244.644 12.4829 Q244.644 14.5589 245.646 15.7135 Q246.649 16.8578 248.462 16.8578 Q249.272 16.8578 250.062 16.6452 Q250.862 16.4224 251.652 15.9869 L251.652 17.7085 Q250.872 18.0731 250.032 18.2554 Q249.201 18.4377 248.259 18.4377 Q245.697 18.4377 244.188 16.8275 Q242.679 15.2172 242.679 12.4829 Q242.679 9.70799 244.198 8.11801 Q245.727 6.52803 248.381 6.52803 Q249.242 6.52803 250.062 6.71032 Q250.882 6.88249 251.652 7.23694 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M260.179 8.54336 Q259.865 8.36107 259.49 8.28005 Q259.126 8.1889 258.68 8.1889 Q257.1 8.1889 256.25 9.22188 Q255.409 10.2447 255.409 12.1689 L255.409 18.144 L253.536 18.144 L253.536 6.80147 L255.409 6.80147 L255.409 8.56361 Q255.997 7.53063 256.938 7.0344 Q257.88 6.52803 259.227 6.52803 Q259.42 6.52803 259.652 6.55841 Q259.885 6.57867 260.169 6.6293 L260.179 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M262.134 6.80147 L263.997 6.80147 L263.997 18.144 L262.134 18.144 L262.134 6.80147 M262.134 2.38598 L263.997 2.38598 L263.997 4.74563 L262.134 4.74563 L262.134 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M267.795 3.581 L267.795 6.80147 L271.633 6.80147 L271.633 8.24967 L267.795 8.24967 L267.795 14.407 Q267.795 15.7945 268.169 16.1894 Q268.554 16.5844 269.719 16.5844 L271.633 16.5844 L271.633 18.144 L269.719 18.144 Q267.562 18.144 266.742 17.3439 Q265.921 16.5338 265.921 14.407 L265.921 8.24967 L264.554 8.24967 L264.554 6.80147 L265.921 6.80147 L265.921 3.581 L267.795 3.581 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M283.289 12.0069 L283.289 12.9183 L274.722 12.9183 Q274.843 14.8425 275.876 15.8552 Q276.919 16.8578 278.773 16.8578 Q279.846 16.8578 280.849 16.5945 Q281.862 16.3312 282.854 15.8046 L282.854 17.5667 Q281.851 17.9921 280.798 18.2149 Q279.745 18.4377 278.661 18.4377 Q275.947 18.4377 274.357 16.8578 Q272.777 15.278 272.777 12.5841 Q272.777 9.79914 274.276 8.16865 Q275.785 6.52803 278.337 6.52803 Q280.626 6.52803 281.953 8.00661 Q283.289 9.47506 283.289 12.0069 M281.426 11.46 Q281.406 9.93079 280.565 9.01934 Q279.735 8.10788 278.358 8.10788 Q276.798 8.10788 275.856 8.98896 Q274.924 9.87003 274.783 11.4701 L281.426 11.46 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M291.817 8.54336 Q291.503 8.36107 291.128 8.28005 Q290.763 8.1889 290.318 8.1889 Q288.738 8.1889 287.887 9.22188 Q287.047 10.2447 287.047 12.1689 L287.047 18.144 L285.173 18.144 L285.173 6.80147 L287.047 6.80147 L287.047 8.56361 Q287.634 7.53063 288.576 7.0344 Q289.518 6.52803 290.865 6.52803 Q291.057 6.52803 291.29 6.55841 Q291.523 6.57867 291.807 6.6293 L291.817 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M293.771 6.80147 L295.635 6.80147 L295.635 18.144 L293.771 18.144 L293.771 6.80147 M293.771 2.38598 L295.635 2.38598 L295.635 4.74563 L293.771 4.74563 L293.771 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M302.744 12.4424 Q300.486 12.4424 299.615 12.9588 Q298.744 13.4753 298.744 14.721 Q298.744 15.7135 299.392 16.3008 Q300.05 16.8781 301.174 16.8781 Q302.724 16.8781 303.655 15.7843 Q304.597 14.6805 304.597 12.8576 L304.597 12.4424 L302.744 12.4424 M306.461 11.6727 L306.461 18.144 L304.597 18.144 L304.597 16.4224 Q303.959 17.4553 303.007 17.9516 Q302.055 18.4377 300.678 18.4377 Q298.936 18.4377 297.903 17.4655 Q296.88 16.4831 296.88 14.8425 Q296.88 12.9285 298.156 11.9562 Q299.442 10.984 301.984 10.984 L304.597 10.984 L304.597 10.8017 Q304.597 9.51557 303.747 8.81679 Q302.906 8.10788 301.377 8.10788 Q300.405 8.10788 299.483 8.34081 Q298.561 8.57374 297.711 9.03959 L297.711 7.31796 Q298.734 6.923 299.696 6.73058 Q300.658 6.52803 301.569 6.52803 Q304.03 6.52803 305.245 7.80407 Q306.461 9.0801 306.461 11.6727 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M323.15 12.4829 Q323.15 10.427 322.3 9.26239 Q321.459 8.08763 319.981 8.08763 Q318.502 8.08763 317.651 9.26239 Q316.811 10.427 316.811 12.4829 Q316.811 14.5387 317.651 15.7135 Q318.502 16.8781 319.981 16.8781 Q321.459 16.8781 322.3 15.7135 Q323.15 14.5387 323.15 12.4829 M316.811 8.5231 Q317.398 7.51038 318.289 7.02427 Q319.191 6.52803 320.436 6.52803 Q322.502 6.52803 323.788 8.16865 Q325.085 9.80926 325.085 12.4829 Q325.085 15.1565 323.788 16.7971 Q322.502 18.4377 320.436 18.4377 Q319.191 18.4377 318.289 17.9516 Q317.398 17.4553 316.811 16.4426 L316.811 18.144 L314.937 18.144 L314.937 2.38598 L316.811 2.38598 L316.811 8.5231 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M331.759 19.1972 Q330.969 21.2227 330.219 21.8405 Q329.47 22.4582 328.214 22.4582 L326.725 22.4582 L326.725 20.8986 L327.819 20.8986 Q328.589 20.8986 329.014 20.534 Q329.439 20.1695 329.956 18.8124 L330.29 17.9617 L325.702 6.80147 L327.677 6.80147 L331.222 15.6729 L334.766 6.80147 L336.741 6.80147 L331.759 19.1972 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M354.717 11.298 L354.717 18.144 L352.854 18.144 L352.854 11.3587 Q352.854 9.7485 352.226 8.94845 Q351.598 8.14839 350.342 8.14839 Q348.833 8.14839 347.962 9.11048 Q347.091 10.0726 347.091 11.7334 L347.091 18.144 L345.218 18.144 L345.218 6.80147 L347.091 6.80147 L347.091 8.56361 Q347.76 7.54076 348.661 7.0344 Q349.572 6.52803 350.757 6.52803 Q352.712 6.52803 353.714 7.7433 Q354.717 8.94845 354.717 11.298 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M356.479 13.6678 L356.479 6.80147 L358.343 6.80147 L358.343 13.5969 Q358.343 15.2071 358.971 16.0173 Q359.598 16.8173 360.854 16.8173 Q362.363 16.8173 363.234 15.8552 Q364.115 14.8931 364.115 13.2323 L364.115 6.80147 L365.979 6.80147 L365.979 18.144 L364.115 18.144 L364.115 16.4021 Q363.437 17.4351 362.535 17.9415 Q361.644 18.4377 360.459 18.4377 Q358.505 18.4377 357.492 17.2224 Q356.479 16.0071 356.479 13.6678 M361.168 6.52803 L361.168 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M376.764 8.97883 Q377.463 7.72305 378.435 7.12554 Q379.407 6.52803 380.724 6.52803 Q382.496 6.52803 383.458 7.77369 Q384.42 9.00921 384.42 11.298 L384.42 18.144 L382.547 18.144 L382.547 11.3587 Q382.547 9.72825 381.97 8.93832 Q381.392 8.14839 380.207 8.14839 Q378.759 8.14839 377.919 9.11048 Q377.078 10.0726 377.078 11.7334 L377.078 18.144 L375.205 18.144 L375.205 11.3587 Q375.205 9.71812 374.627 8.93832 Q374.05 8.14839 372.845 8.14839 Q371.417 8.14839 370.576 9.12061 Q369.736 10.0827 369.736 11.7334 L369.736 18.144 L367.862 18.144 L367.862 6.80147 L369.736 6.80147 L369.736 8.56361 Q370.374 7.5205 371.265 7.02427 Q372.156 6.52803 373.382 6.52803 Q374.617 6.52803 375.478 7.15592 Q376.349 7.78381 376.764 8.97883 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M394.517 12.4829 Q394.517 10.427 393.667 9.26239 Q392.826 8.08763 391.347 8.08763 Q389.869 8.08763 389.018 9.26239 Q388.178 10.427 388.178 12.4829 Q388.178 14.5387 389.018 15.7135 Q389.869 16.8781 391.347 16.8781 Q392.826 16.8781 393.667 15.7135 Q394.517 14.5387 394.517 12.4829 M388.178 8.5231 Q388.765 7.51038 389.656 7.02427 Q390.557 6.52803 391.803 6.52803 Q393.869 6.52803 395.155 8.16865 Q396.452 9.80926 396.452 12.4829 Q396.452 15.1565 395.155 16.7971 Q393.869 18.4377 391.803 18.4377 Q390.557 18.4377 389.656 17.9516 Q388.765 17.4553 388.178 16.4426 L388.178 18.144 L386.304 18.144 L386.304 2.38598 L388.178 2.38598 L388.178 8.5231 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M408.108 12.0069 L408.108 12.9183 L399.54 12.9183 Q399.662 14.8425 400.695 15.8552 Q401.738 16.8578 403.591 16.8578 Q404.665 16.8578 405.667 16.5945 Q406.68 16.3312 407.673 15.8046 L407.673 17.5667 Q406.67 17.9921 405.617 18.2149 Q404.563 18.4377 403.48 18.4377 Q400.766 18.4377 399.176 16.8578 Q397.596 15.278 397.596 12.5841 Q397.596 9.79914 399.095 8.16865 Q400.604 6.52803 403.156 6.52803 Q405.445 6.52803 406.771 8.00661 Q408.108 9.47506 408.108 12.0069 M406.245 11.46 Q406.224 9.93079 405.384 9.01934 Q404.553 8.10788 403.176 8.10788 Q401.616 8.10788 400.675 8.98896 Q399.743 9.87003 399.601 11.4701 L406.245 11.46 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M416.635 8.54336 Q416.321 8.36107 415.946 8.28005 Q415.582 8.1889 415.136 8.1889 Q413.556 8.1889 412.706 9.22188 Q411.865 10.2447 411.865 12.1689 L411.865 18.144 L409.992 18.144 L409.992 6.80147 L411.865 6.80147 L411.865 8.56361 Q412.453 7.53063 413.394 7.0344 Q414.336 6.52803 415.683 6.52803 Q415.876 6.52803 416.109 6.55841 Q416.341 6.57867 416.625 6.6293 L416.635 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M429.578 8.10788 Q428.079 8.10788 427.208 9.28265 Q426.337 10.4473 426.337 12.4829 Q426.337 14.5184 427.198 15.6932 Q428.069 16.8578 429.578 16.8578 Q431.066 16.8578 431.937 15.6831 Q432.808 14.5083 432.808 12.4829 Q432.808 10.4675 431.937 9.29277 Q431.066 8.10788 429.578 8.10788 M429.578 6.52803 Q432.008 6.52803 433.396 8.10788 Q434.783 9.68774 434.783 12.4829 Q434.783 15.2679 433.396 16.8578 Q432.008 18.4377 429.578 18.4377 Q427.137 18.4377 425.75 16.8578 Q424.372 15.2679 424.372 12.4829 Q424.372 9.68774 425.75 8.10788 Q427.137 6.52803 429.578 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M442.48 2.38598 L442.48 3.93545 L440.698 3.93545 Q439.695 3.93545 439.3 4.34054 Q438.915 4.74563 438.915 5.79887 L438.915 6.80147 L441.984 6.80147 L441.984 8.24967 L438.915 8.24967 L438.915 18.144 L437.042 18.144 L437.042 8.24967 L435.259 8.24967 L435.259 6.80147 L437.042 6.80147 L437.042 6.01154 Q437.042 4.11774 437.923 3.25693 Q438.804 2.38598 440.718 2.38598 L442.48 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M459.19 7.23694 L459.19 8.97883 Q458.4 8.54336 457.6 8.33068 Q456.81 8.10788 456 8.10788 Q454.187 8.10788 453.184 9.26239 Q452.182 10.4068 452.182 12.4829 Q452.182 14.5589 453.184 15.7135 Q454.187 16.8578 456 16.8578 Q456.81 16.8578 457.6 16.6452 Q458.4 16.4224 459.19 15.9869 L459.19 17.7085 Q458.41 18.0731 457.57 18.2554 Q456.739 18.4377 455.797 18.4377 Q453.235 18.4377 451.726 16.8275 Q450.217 15.2172 450.217 12.4829 Q450.217 9.70799 451.736 8.11801 Q453.265 6.52803 455.919 6.52803 Q456.78 6.52803 457.6 6.71032 Q458.42 6.88249 459.19 7.23694 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M461.144 2.38598 L463.008 2.38598 L463.008 18.144 L461.144 18.144 L461.144 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M470.117 12.4424 Q467.859 12.4424 466.988 12.9588 Q466.117 13.4753 466.117 14.721 Q466.117 15.7135 466.765 16.3008 Q467.423 16.8781 468.547 16.8781 Q470.097 16.8781 471.029 15.7843 Q471.97 14.6805 471.97 12.8576 L471.97 12.4424 L470.117 12.4424 M473.834 11.6727 L473.834 18.144 L471.97 18.144 L471.97 16.4224 Q471.332 17.4553 470.381 17.9516 Q469.429 18.4377 468.051 18.4377 Q466.309 18.4377 465.276 17.4655 Q464.254 16.4831 464.254 14.8425 Q464.254 12.9285 465.53 11.9562 Q466.816 10.984 469.358 10.984 L471.97 10.984 L471.97 10.8017 Q471.97 9.51557 471.12 8.81679 Q470.279 8.10788 468.75 8.10788 Q467.778 8.10788 466.856 8.34081 Q465.935 8.57374 465.084 9.03959 L465.084 7.31796 Q466.107 6.923 467.069 6.73058 Q468.031 6.52803 468.942 6.52803 Q471.403 6.52803 472.619 7.80407 Q473.834 9.0801 473.834 11.6727 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M483.019 7.13567 L483.019 8.89781 Q482.229 8.49272 481.379 8.29018 Q480.528 8.08763 479.617 8.08763 Q478.229 8.08763 477.53 8.51298 Q476.842 8.93832 476.842 9.78901 Q476.842 10.4372 477.338 10.8119 Q477.834 11.1764 479.333 11.5106 L479.971 11.6524 Q481.956 12.0778 482.786 12.8576 Q483.627 13.6272 483.627 15.0147 Q483.627 16.5945 482.371 17.5161 Q481.126 18.4377 478.938 18.4377 Q478.027 18.4377 477.034 18.2554 Q476.052 18.0832 474.958 17.7288 L474.958 15.8046 Q475.991 16.3413 476.994 16.6148 Q477.996 16.8781 478.979 16.8781 Q480.295 16.8781 481.004 16.4325 Q481.713 15.9768 481.713 15.1565 Q481.713 14.3969 481.196 13.9918 Q480.69 13.5867 478.958 13.212 L478.31 13.0601 Q476.578 12.6955 475.809 11.9461 Q475.039 11.1866 475.039 9.87003 Q475.039 8.26992 476.173 7.39898 Q477.308 6.52803 479.394 6.52803 Q480.427 6.52803 481.338 6.67994 Q482.25 6.83185 483.019 7.13567 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M492.812 7.13567 L492.812 8.89781 Q492.022 8.49272 491.172 8.29018 Q490.321 8.08763 489.41 8.08763 Q488.022 8.08763 487.323 8.51298 Q486.635 8.93832 486.635 9.78901 Q486.635 10.4372 487.131 10.8119 Q487.627 11.1764 489.126 11.5106 L489.764 11.6524 Q491.749 12.0778 492.579 12.8576 Q493.42 13.6272 493.42 15.0147 Q493.42 16.5945 492.164 17.5161 Q490.919 18.4377 488.731 18.4377 Q487.82 18.4377 486.827 18.2554 Q485.845 18.0832 484.751 17.7288 L484.751 15.8046 Q485.784 16.3413 486.787 16.6148 Q487.789 16.8781 488.772 16.8781 Q490.088 16.8781 490.797 16.4325 Q491.506 15.9768 491.506 15.1565 Q491.506 14.3969 490.989 13.9918 Q490.483 13.5867 488.751 13.212 L488.103 13.0601 Q486.371 12.6955 485.602 11.9461 Q484.832 11.1866 484.832 9.87003 Q484.832 8.26992 485.966 7.39898 Q487.101 6.52803 489.187 6.52803 Q490.22 6.52803 491.131 6.67994 Q492.043 6.83185 492.812 7.13567 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M505.076 12.0069 L505.076 12.9183 L496.509 12.9183 Q496.63 14.8425 497.663 15.8552 Q498.706 16.8578 500.56 16.8578 Q501.633 16.8578 502.636 16.5945 Q503.649 16.3312 504.641 15.8046 L504.641 17.5667 Q503.638 17.9921 502.585 18.2149 Q501.532 18.4377 500.448 18.4377 Q497.734 18.4377 496.144 16.8578 Q494.564 15.278 494.564 12.5841 Q494.564 9.79914 496.063 8.16865 Q497.572 6.52803 500.124 6.52803 Q502.413 6.52803 503.74 8.00661 Q505.076 9.47506 505.076 12.0069 M503.213 11.46 Q503.193 9.93079 502.352 9.01934 Q501.522 8.10788 500.145 8.10788 Q498.585 8.10788 497.643 8.98896 Q496.711 9.87003 496.57 11.4701 L503.213 11.46 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M514.262 7.13567 L514.262 8.89781 Q513.472 8.49272 512.621 8.29018 Q511.771 8.08763 510.859 8.08763 Q509.472 8.08763 508.773 8.51298 Q508.084 8.93832 508.084 9.78901 Q508.084 10.4372 508.581 10.8119 Q509.077 11.1764 510.576 11.5106 L511.214 11.6524 Q513.199 12.0778 514.029 12.8576 Q514.87 13.6272 514.87 15.0147 Q514.87 16.5945 513.614 17.5161 Q512.368 18.4377 510.181 18.4377 Q509.269 18.4377 508.277 18.2554 Q507.294 18.0832 506.201 17.7288 L506.201 15.8046 Q507.234 16.3413 508.236 16.6148 Q509.239 16.8781 510.221 16.8781 Q511.538 16.8781 512.247 16.4325 Q512.956 15.9768 512.956 15.1565 Q512.956 14.3969 512.439 13.9918 Q511.933 13.5867 510.201 13.212 L509.553 13.0601 Q507.821 12.6955 507.051 11.9461 Q506.282 11.1866 506.282 9.87003 Q506.282 8.26992 507.416 7.39898 Q508.55 6.52803 510.636 6.52803 Q511.669 6.52803 512.581 6.67994 Q513.492 6.83185 514.262 7.13567 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip022)" style="stroke:#009af9; stroke-width:1; stroke-opacity:1; fill:none" points="
  72.1496,40.0641 239.153,258.475 406.156,237.74 573.159,189.919 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#e26f46; stroke-width:1; stroke-opacity:1; fill:none" points="
  72.1496,74.7009 239.153,330.222 406.156,346.599 573.159,335.889 
  "/>
<path clip-path="url(#clip020)" d="
M493.597 87.0589 L570.487 87.0589 L570.487 41.6989 L493.597 41.6989  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  493.597,87.0589 570.487,87.0589 570.487,41.6989 493.597,41.6989 493.597,87.0589 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#009af9; stroke-width:1; stroke-opacity:1; fill:none" points="
  499.498,56.8189 534.902,56.8189 
  "/>
<path clip-path="url(#clip020)" d="M 0 0 M541.972 57.0128 L541.972 60.1783 L543.847 60.1783 Q544.791 60.1783 545.242 59.7906 Q545.699 59.397 545.699 58.5926 Q545.699 57.7825 545.242 57.4005 Q544.791 57.0128 543.847 57.0128 L541.972 57.0128 M541.972 53.4596 L541.972 56.0637 L543.703 56.0637 Q544.559 56.0637 544.976 55.7454 Q545.398 55.4214 545.398 54.7616 Q545.398 54.1077 544.976 53.7836 Q544.559 53.4596 543.703 53.4596 L541.972 53.4596 M540.803 52.4989 L543.789 52.4989 Q545.126 52.4989 545.85 53.0545 Q546.573 53.61 546.573 54.6343 Q546.573 55.4272 546.203 55.8959 Q545.832 56.3646 545.115 56.4804 Q545.977 56.6656 546.451 57.2558 Q546.932 57.8403 546.932 58.72 Q546.932 59.8774 546.145 60.5081 Q545.358 61.1389 543.905 61.1389 L540.803 61.1389 L540.803 52.4989 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M548.095 52.4989 L549.264 52.4989 L549.264 61.1389 L548.095 61.1389 L548.095 52.4989 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M556.897 53.1644 L556.897 54.3971 Q556.307 53.8473 555.635 53.5753 Q554.97 53.3033 554.218 53.3033 Q552.736 53.3033 551.949 54.2119 Q551.162 55.1147 551.162 56.8276 Q551.162 58.5348 551.949 59.4433 Q552.736 60.3461 554.218 60.3461 Q554.97 60.3461 555.635 60.0741 Q556.307 59.8021 556.897 59.2524 L556.897 60.4734 Q556.283 60.8901 555.595 61.0984 Q554.912 61.3068 554.148 61.3068 Q552.186 61.3068 551.058 60.1088 Q549.929 58.9051 549.929 56.8276 Q549.929 54.7443 551.058 53.5464 Q552.186 52.3427 554.148 52.3427 Q554.924 52.3427 555.606 52.551 Q556.295 52.7536 556.897 53.1644 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip020)" style="stroke:#e26f46; stroke-width:1; stroke-opacity:1; fill:none" points="
  499.498,71.9389 534.902,71.9389 
  "/>
<path clip-path="url(#clip020)" d="M 0 0 M544.762 68.7705 L543.176 73.0703 L546.353 73.0703 L544.762 68.7705 M544.102 67.6189 L545.427 67.6189 L548.72 76.2589 L547.505 76.2589 L546.718 74.0425 L542.823 74.0425 L542.036 76.2589 L540.803 76.2589 L544.102 67.6189 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M549.883 67.6189 L551.052 67.6189 L551.052 76.2589 L549.883 76.2589 L549.883 67.6189 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M 0 0 M558.685 68.2844 L558.685 69.5171 Q558.095 68.9673 557.424 68.6953 Q556.758 68.4233 556.006 68.4233 Q554.524 68.4233 553.737 69.3319 Q552.95 70.2347 552.95 71.9476 Q552.95 73.6548 553.737 74.5633 Q554.524 75.4661 556.006 75.4661 Q556.758 75.4661 557.424 75.1941 Q558.095 74.9221 558.685 74.3724 L558.685 75.5934 Q558.072 76.0101 557.383 76.2184 Q556.7 76.4268 555.936 76.4268 Q553.974 76.4268 552.846 75.2288 Q551.718 74.0251 551.718 71.9476 Q551.718 69.8643 552.846 68.6664 Q553.974 67.4627 555.936 67.4627 Q556.712 67.4627 557.395 67.671 Q558.083 67.8736 558.685 68.2844 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<p>We see that following the &quot;lowest AIC&quot; rule we would indeed choose three classes, while following the &quot;best AIC&quot; criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and opur models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).</p><p>We could study this issue more in detail by analysing the <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a>, but the one used in BetaML does not account for the ignoreLabels option (yet).</p><h2 id="Benchmarking-computational-efficiency"><a class="docs-heading-anchor" href="#Benchmarking-computational-efficiency">Benchmarking computational efficiency</a><a id="Benchmarking-computational-efficiency-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarking-computational-efficiency" title="Permalink"></a></h2><p>We now benchmark the time and memory required by the various models by using the <code>@btime</code> macro of the <code>BenchmarkTools</code> package:</p><pre><code class="nohighlight hljs">@btime kmeans($xs,3);
# 261.540 μs (3777 allocations: 442.53 KiB)
@btime kmedoids($xs,3);
4.576 ms (97356 allocations: 10.42 MiB)
@btime gmm($xs,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE);
# 5.498 ms (133365 allocations: 8.42 MiB)
@btime gmm($xs,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE);
# 18.901 ms (404333 allocations: 25.65 MiB)
@btime gmm($xs,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE);
# 49.257 ms (351500 allocations: 61.95 MiB)
@btime Clustering.kmeans($xs&#39;, 3);
# 17.071 μs (23 allocations: 14.31 KiB)
@btime begin dGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:diag); GaussianMixtures.em!(dGMM, $xs) end;
# 530.528 μs (2088 allocations: 488.05 KiB)
@btime begin fGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:full); GaussianMixtures.em!(fGMM, $xs) end;
# 4.166 ms (58910 allocations: 3.59 MiB)</code></pre><p>(<em>note: the values reported here are of a local pc, not of the GitHub CI server, as sometimes - depending on data and random initialisation - <code>GaussainMixtures.em!</code><code>fails with a</code>PosDefException`. This in turn would lead the whole documentation to fail to compile</em>)</p><p>Like for supervised models, dedicated models are much better optimized than BetaML models, and are order of magnitude more efficient. However even the slowest BetaML clusering model (gmm using full gaussians) is realtively fast and can handle mid-size datasets (tens to hundreds of thousand records) without significant slow downs.</p><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>We have shown in this tutorial how we can easily run clustering almgorithms in BetaML with just one line of code <code>choosenModel(x,k)</code>, but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to mid-size datasets. However we trade this relative inefficiency with very flexible model definition and utility functions (for example the BetaML gmm works with missing data, allowing it to be used as the backbone of the <a href="../../Clustering.html#BetaML.Clustering.predictMissing"><code>predictMissing</code></a> missing imputation function, or for collaborative reccomendation systems).</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Classification - cars/betaml_tutorial_classification_cars.html">« A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a class="docs-footer-nextpage" href="../../Perceptron.html">Perceptron »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Monday 27 June 2022 16:12">Monday 27 June 2022</span>. Using Julia version 1.6.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
